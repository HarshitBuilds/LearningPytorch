{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook to learn the basics and fundamentals of PyTorch. Self taught and coded and with the help of Youtube lectures."
      ],
      "metadata": {
        "id": "5GDwkyQOhKuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R3sGW9LLhZkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Tensor Basics\n",
        "import torch\n",
        "x = torch.empty(1) #scalar\n",
        "print(x)\n",
        "x = torch.empty(3) #vector(1D) with 3 values\n",
        "print(x)\n",
        "x = torch.empty(2,2,2,2) #4 dimensional tensor with random values\n",
        "print(x)\n",
        "x = torch.rand(3,3) #initialises random values in the tensor\n",
        "x = torch.rand(4,2) # 4 rows and 2 columns\n",
        "print(x)\n",
        "x = torch.ones(2,2,2) #initialises the tensor with ones"
      ],
      "metadata": {
        "id": "XRrGgMU4kZ2j",
        "outputId": "56ebc63d-bfd5-4e9b-960a-11ba78b60dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.4506e+28])\n",
            "tensor([-1.4506e+28,  4.4118e-41,  3.5272e+02])\n",
            "tensor([[[[ 4.3457e+01,  0.0000e+00],\n",
            "          [-9.2821e+23,  4.4117e-41]],\n",
            "\n",
            "         [[ 0.0000e+00,  0.0000e+00],\n",
            "          [ 0.0000e+00,  0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000e+00,  0.0000e+00],\n",
            "          [ 0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "         [[ 0.0000e+00,  0.0000e+00],\n",
            "          [ 0.0000e+00,  0.0000e+00]]]])\n",
            "tensor([[0.2612, 0.3955],\n",
            "        [0.6821, 0.2630],\n",
            "        [0.2755, 0.9961],\n",
            "        [0.6179, 0.5675]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.dtype #prints the data type\n",
        "x = torch.ones(2,2,dtype=torch.float64) #can change the data type of the tensor\n",
        "print(x.dtype)\n",
        "print(x.size())\n",
        "a = torch.tensor([[[56,32],[45,76]]])\n",
        "print(a.shape)"
      ],
      "metadata": {
        "id": "i_6g9KdMkrir",
        "outputId": "17a9e46c-56b6-4e38-b942-c9cf0f0456be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.Size([2, 2])\n",
            "torch.Size([1, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([4,5]) #Creating a 1D tensor with custom values\n",
        "print(x)\n",
        "print(x.shape) #.shape returns a tuple containing the size of the tensor.\n",
        "a = torch.tensor([[[56,32],[45,76]]]) #so in this case the tupple is [1,2,2]\n",
        "print(a.shape)\n",
        "print(a.shape[0], a.shape[1], a.shape[2])\n",
        "#print(a.shape[4]) #will give an error as there are only 3 elements in this tupple"
      ],
      "metadata": {
        "id": "jl3PSgi74vuv",
        "outputId": "ba9c928a-eb6c-402d-b7f5-5e67c974003a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 5])\n",
            "torch.Size([2])\n",
            "torch.Size([1, 2, 2])\n",
            "1 2 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(3,3)\n",
        "b = torch.rand(3,3)\n",
        "print(a)\n",
        "print(b)\n",
        "c = a + b #element wise addition\n",
        "#or\n",
        "c = torch.add(a,b) #element wise addition\n",
        "print(c)\n",
        "a.add_(1) #adds one to each element of the tensor\n",
        "b.add_(a) #in place element wise addition. Modifies b as well.\n",
        "#In pytorch function followed by \"_\" is used for inplace operation (i.e modifies the original as well)\n",
        "c = torch.sub(a,b) #element wise subtraction\n",
        "c = torch.mul(a,b) #element wise multiplication\n",
        "c = torch.div(a,b) #element wise division"
      ],
      "metadata": {
        "id": "7pRzlWOq5X4W",
        "outputId": "158afcdb-107e-48d7-ae60-c75b429beaff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8605, 0.7791, 0.1740],\n",
            "        [0.9716, 0.5073, 0.6938],\n",
            "        [0.0954, 0.7237, 0.6132]])\n",
            "tensor([[0.1200, 0.0459, 0.0075],\n",
            "        [0.3836, 0.2929, 0.6005],\n",
            "        [0.1762, 0.2318, 0.0612]])\n",
            "tensor([[0.9806, 0.8250, 0.1815],\n",
            "        [1.3551, 0.8001, 1.2942],\n",
            "        [0.2716, 0.9555, 0.6745]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9806, 0.8250, 0.1815],\n",
              "        [1.3551, 0.8001, 1.2942],\n",
              "        [0.2716, 0.9555, 0.6745]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,5)\n",
        "print(x)\n",
        "print(x[:,2]) #prints all the rows and the third column\n",
        "print(x[2,:]) #prints all the third row and all the columns\n",
        "print(x[1,1]) #for printing a specific element as a tensor\n",
        "print(x[1,1].item()) #prints only the value. Command only valid for a single element"
      ],
      "metadata": {
        "id": "s5qR1rAj6bGN",
        "outputId": "cf9f1efa-5068-4df5-93d4-928b45f79e48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0822, 0.0459, 0.1931, 0.9215, 0.0313],\n",
            "        [0.6801, 0.0157, 0.8164, 0.4154, 0.2698],\n",
            "        [0.1590, 0.0174, 0.0883, 0.9070, 0.3025],\n",
            "        [0.5032, 0.0669, 0.6232, 0.7293, 0.0554]])\n",
            "tensor([0.1931, 0.8164, 0.0883, 0.6232])\n",
            "tensor([0.1590, 0.0174, 0.0883, 0.9070, 0.3025])\n",
            "tensor(0.0157)\n",
            "0.015674054622650146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(6,6)\n",
        "print(x)\n",
        "y = x.view(36) #reshaping the tensor to a 1D vector\n",
        "print(y)\n",
        "y.view_as(x) #reshapes one tensor (y) to another tensor (x)\n",
        "y = x.view(-1,4) #pytorch automatically calculates the other dimension (9 rows and 4 columns)\n",
        "print(y)\n",
        "print(y.size())"
      ],
      "metadata": {
        "id": "QHZ1Ub_b8EC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a5cf5a-293b-40a0-88ca-64a6d65ba9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3309, 0.4526, 0.7162, 0.2089, 0.4257, 0.3912],\n",
            "        [0.1145, 0.2543, 0.9976, 0.1835, 0.4800, 0.5791],\n",
            "        [0.5704, 0.7061, 0.1702, 0.9646, 0.9614, 0.1373],\n",
            "        [0.2024, 0.0435, 0.2956, 0.6521, 0.1169, 0.1637],\n",
            "        [0.5576, 0.1297, 0.1464, 0.9769, 0.6393, 0.6266],\n",
            "        [0.8071, 0.9964, 0.1020, 0.9383, 0.9355, 0.5240]])\n",
            "tensor([0.3309, 0.4526, 0.7162, 0.2089, 0.4257, 0.3912, 0.1145, 0.2543, 0.9976,\n",
            "        0.1835, 0.4800, 0.5791, 0.5704, 0.7061, 0.1702, 0.9646, 0.9614, 0.1373,\n",
            "        0.2024, 0.0435, 0.2956, 0.6521, 0.1169, 0.1637, 0.5576, 0.1297, 0.1464,\n",
            "        0.9769, 0.6393, 0.6266, 0.8071, 0.9964, 0.1020, 0.9383, 0.9355, 0.5240])\n",
            "tensor([[0.3309, 0.4526, 0.7162, 0.2089],\n",
            "        [0.4257, 0.3912, 0.1145, 0.2543],\n",
            "        [0.9976, 0.1835, 0.4800, 0.5791],\n",
            "        [0.5704, 0.7061, 0.1702, 0.9646],\n",
            "        [0.9614, 0.1373, 0.2024, 0.0435],\n",
            "        [0.2956, 0.6521, 0.1169, 0.1637],\n",
            "        [0.5576, 0.1297, 0.1464, 0.9769],\n",
            "        [0.6393, 0.6266, 0.8071, 0.9964],\n",
            "        [0.1020, 0.9383, 0.9355, 0.5240]])\n",
            "torch.Size([9, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = torch.rand(64)\n",
        "print(a)\n",
        "b = a.numpy() #Converts to numpy array\n",
        "#If both the numpy array and tensor are on CPU then modifying one also modfies the other"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue9OZwvz9Srk",
        "outputId": "3d101eef-129a-4de3-ddbd-727e01c766bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3228, 0.8968, 0.4660, 0.5868, 0.1761, 0.9147, 0.7201, 0.3227, 0.9408,\n",
            "        0.1993, 0.1180, 0.9326, 0.1994, 0.0361, 0.8788, 0.9226, 0.4326, 0.2759,\n",
            "        0.3155, 0.0276, 0.5616, 0.8059, 0.4517, 0.3782, 0.0952, 0.5925, 0.8004,\n",
            "        0.8003, 0.8850, 0.5722, 0.7951, 0.4122, 0.5272, 0.2309, 0.3985, 0.4181,\n",
            "        0.2316, 0.8064, 0.0874, 0.6976, 0.8313, 0.0680, 0.6019, 0.0180, 0.5130,\n",
            "        0.7635, 0.0900, 0.3764, 0.1201, 0.9350, 0.6083, 0.4424, 0.1034, 0.0802,\n",
            "        0.5934, 0.3059, 0.2110, 0.2351, 0.8305, 0.2229, 0.0659, 0.6486, 0.5540,\n",
            "        0.0219])\n",
            "[0.32279563 0.8967894  0.46597403 0.5868273  0.17610067 0.9146926\n",
            " 0.7200838  0.32272267 0.940773   0.19927621 0.11801422 0.93257135\n",
            " 0.1993599  0.03610539 0.87875426 0.9225503  0.4325717  0.27593547\n",
            " 0.315502   0.02757502 0.56155103 0.80589527 0.45170045 0.37823743\n",
            " 0.09522653 0.59252524 0.80043894 0.800259   0.8849732  0.57223636\n",
            " 0.79510444 0.4121536  0.5271538  0.23093665 0.39851618 0.41806304\n",
            " 0.23159832 0.806372   0.08739346 0.6976451  0.83132505 0.0680244\n",
            " 0.601876   0.01796407 0.5129753  0.7635253  0.08999884 0.37644255\n",
            " 0.12005603 0.9350243  0.6082985  0.44236135 0.10341656 0.08024251\n",
            " 0.59342027 0.3058812  0.21097445 0.23514134 0.8304949  0.22285801\n",
            " 0.06586945 0.64856416 0.554017   0.02188033]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros(5)\n",
        "print(a)\n",
        "b = torch.from_numpy(a)\n",
        "print(b)\n",
        "a +=1\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VB813DLCs_-",
        "outputId": "7027b8d6-49fa-4096-ffcd-7aabc004e170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0.]\n",
            "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  print(\"yes\")\n",
        "  device = torch.device(\"cuda\")\n",
        "  x = torch.ones(5) #create a tensor\n",
        "  x = x.to(device) #moves the tensor to GPU\n",
        "  x = x.to(\"cpu\") #moves it back to CPU\n",
        "else:\n",
        "  print(\"no\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opxh6eJ8Dw-t",
        "outputId": "5551e64b-1b94-42d6-cdc5-43d98d96122c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(6, requires_grad=True) #by default false, states that gradient will need to be evaluated wrt this variable"
      ],
      "metadata": {
        "id": "KTT4pPvQEB5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#autograds\n",
        "import torch\n",
        "x = torch.rand(5,requires_grad=True) #buids a computation graph for this variable\n",
        "print(x)\n",
        "y = x+5\n",
        "print(y)\n",
        "z = y*y*5\n",
        "z = z.mean() #converts the tensor back to a scalar\n",
        "#y.backward() will give an error as y is not a scalar.\n",
        "z.backward() #computes dz/dx\n",
        "print(x.grad) #partial derivatives wrt to all the 5 elements in x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdIWPzgQDXLb",
        "outputId": "871e1523-5a5d-44d8-a8da-02e575cbf4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2744, 0.0195, 0.1935, 0.9088, 0.2416], requires_grad=True)\n",
            "tensor([5.2744, 5.0195, 5.1935, 5.9088, 5.2416], grad_fn=<AddBackward0>)\n",
            "tensor([10.5488, 10.0389, 10.3869, 11.8176, 10.4833])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = y*y*5\n",
        "#z = z.mean() #commented so z is still a vector\n",
        "#y.backward() will give an error as y is not a scalar.\n",
        "a = torch.tensor([0.1,0.2,0.3,0.4,0.5], dtype=torch.float32)\n",
        "z.backward(a) #telling PyTorch to compute the gradient of the dot product of z and a with respect to x.\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq2qzo-zDXin",
        "outputId": "71ec95fb-76c5-4fbb-dcb0-c33544c2b146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([15.8231, 20.0778, 25.9673, 35.4529, 36.6915])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Preventing computation of gradients, different ways of implementing\n",
        " x.requires_grad_(False) # in place command\n",
        " print(x)\n",
        " y = x.detach() #new tensor with same values without requies_grad feature\n",
        " print(y)\n",
        " x.requires_grad_(True)\n",
        " with torch.no_grad():\n",
        "  z = x + 5\n",
        "  print(z) #z does not have gradient function attribute\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexFt1EmDXzU",
        "outputId": "d3d51a6a-54f3-4d21-d9d8-c6ff2de31bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2744, 0.0195, 0.1935, 0.9088, 0.2416])\n",
            "tensor([0.2744, 0.0195, 0.1935, 0.9088, 0.2416])\n",
            "tensor([5.2744, 5.0195, 5.1935, 5.9088, 5.2416])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch accumulate gradient in every step of .backward() call\n",
        "#that is why it is important to set the .grad tensor with zeros before the next iteration.\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "id": "ZTv4hjiDDX0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Backpropogation for simple linear regession with sum of square loss function\n",
        "x = torch.tensor([1.0])\n",
        "y = torch.tensor([2.0])\n",
        "w = torch.tensor([1.0],requires_grad=True)\n",
        "y_pred = w*x\n",
        "loss = (y - y_pred)**2\n",
        "loss.backward() #computes d(loss)/d(w)\n",
        "print(w.grad) #prints the gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oZU-1_HMeij",
        "outputId": "bcbd59cc-d7cb-4b2f-e77d-d97d88ee5b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing linear regression manually\n",
        "import numpy as np\n",
        "#True function is y = 2*x\n",
        "x = np.array([1,2,3,4],dtype=np.float32)\n",
        "y = np.array([2,4,6,8],dtype=np.float32)\n",
        "w = 0.0 #intial value of weights\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y,y_pred):\n",
        "  return ((y-y_pred)**2).mean() #loss function\n",
        "\n",
        "def gradient(x,y,y_pred):\n",
        "  return (np.multiply(2*x,y_pred-y)).mean() #computing the gradient wrt to the weight\n",
        "\n",
        "print(f'prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "lr = 0.01\n",
        "num_iters = 10\n",
        "for epoch in range(num_iters):\n",
        "   y_pred = forward(x)\n",
        "   l = loss(y,y_pred)\n",
        "   dw = gradient(x,y,y_pred)\n",
        "   w -= lr*(dw)\n",
        "   print(f'epoch: {epoch+1} w: {w:.3f} Training loss: {l:.5f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60F5ptWKMeoi",
        "outputId": "ad9c2736-c40d-40ac-f9fe-bdd48c36c36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0.000\n",
            "epoch: 1 w: 0.300 Training loss: 30.00000\n",
            "epoch: 2 w: 0.555 Training loss: 21.67500\n",
            "epoch: 3 w: 0.772 Training loss: 15.66019\n",
            "epoch: 4 w: 0.956 Training loss: 11.31449\n",
            "epoch: 5 w: 1.113 Training loss: 8.17472\n",
            "epoch: 6 w: 1.246 Training loss: 5.90623\n",
            "epoch: 7 w: 1.359 Training loss: 4.26725\n",
            "epoch: 8 w: 1.455 Training loss: 3.08309\n",
            "epoch: 9 w: 1.537 Training loss: 2.22753\n",
            "epoch: 10 w: 1.606 Training loss: 1.60939\n",
            "prediction after training: f(5) = 8.031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing gradient computation using pytorch instead of manual\n",
        "import numpy as np\n",
        "import torch\n",
        "#True function is y = 2*x\n",
        "x = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8],dtype=torch.float32)\n",
        "w = torch.tensor([0.0], dtype=torch.float32, requires_grad=True) #intial value of weights\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y,y_pred):\n",
        "  return ((y-y_pred)**2).mean() #loss function\n",
        "\n",
        "print(f'prediction before training: f(5) = {forward(5).item():.3f}') #formatted string can't directly print a tensor, you need to get the value of the  tensor.\n",
        "\n",
        "lr = 0.01\n",
        "num_iters = 100\n",
        "for epoch in range(num_iters):\n",
        "   y_pred = forward(x)\n",
        "   l = loss(y,y_pred)\n",
        "   l.backward() #computes the d(loss)/dw\n",
        "   dw = w.grad\n",
        "   with torch.no_grad():\n",
        "      w -= lr*(dw) #don't want to compute the gradients for this part.\n",
        "   w.grad.zero_() #prevents accumulation of gradient before the next iteration.\n",
        "   if epoch % 10 == 0:\n",
        "    print(f'epoch: {epoch+1} w: {w.item():.3f} Training loss: {l:.5f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9KshJTkEMkz",
        "outputId": "a680e093-dc81-4059-ca71-a1c68cf16cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0.000\n",
            "epoch: 1 w: 0.300 Training loss: 30.00000\n",
            "epoch: 11 w: 1.665 Training loss: 1.16279\n",
            "epoch: 21 w: 1.934 Training loss: 0.04507\n",
            "epoch: 31 w: 1.987 Training loss: 0.00175\n",
            "epoch: 41 w: 1.997 Training loss: 0.00007\n",
            "epoch: 51 w: 1.999 Training loss: 0.00000\n",
            "epoch: 61 w: 2.000 Training loss: 0.00000\n",
            "epoch: 71 w: 2.000 Training loss: 0.00000\n",
            "epoch: 81 w: 2.000 Training loss: 0.00000\n",
            "epoch: 91 w: 2.000 Training loss: 0.00000\n",
            "prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Automating computation of loss and paramter updates using Pytorch\n",
        "#Basic pipeline for training part\n",
        "# Forward pass: computing the predictions\n",
        "# Backward pass: Computing the gradients\n",
        "# weight updates\n",
        "# All this in the Training loop\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn #pytorch neural network module\n",
        "\n",
        "#True function is y = 2*x\n",
        "x = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8],dtype=torch.float32)\n",
        "w = torch.tensor([0.0], dtype=torch.float32, requires_grad=True) #intial value of weights\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "loss = nn.MSELoss() #we can call this function in the training loop\n",
        "optimizer = torch.optim.SGD([w],lr=learning_rate)\n",
        "\n",
        "print(f'prediction before training: f(5) = {forward(5).item():.3f}') #formatted string can't directly print a tensor, you need to get the value of the  tensor.\n",
        "\n",
        "num_iters = 100\n",
        "for epoch in range(num_iters):\n",
        "   y_pred = forward(x)\n",
        "   l = loss(y,y_pred)\n",
        "  #  l.backward() #computes the d(loss)/dw\n",
        "  #  dw = w.grad\n",
        "  #  with torch.no_grad():\n",
        "  #     w -= lr*(dw) #don't want to compute the gradients for this part.\n",
        "  #  w.grad.zero_() #prevents accumulation of gradient before the next iteration.\n",
        "   l.backward() #although we do have the same command for computing the gradients.\n",
        "   optimizer.step() # for automating weight updates\n",
        "   optimizer.zero_grad() #zeroing the gradients.\n",
        "\n",
        "   if epoch % 10 == 0:\n",
        "    print(f'epoch: {epoch+1} w: {w.item():.3f} Training loss: {l:.5f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGtRrFasGk_J",
        "outputId": "2b1645a3-43f4-49e2-9b01-b5c678d3d930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0.000\n",
            "epoch: 1 w: 0.300 Training loss: 30.00000\n",
            "epoch: 11 w: 1.665 Training loss: 1.16279\n",
            "epoch: 21 w: 1.934 Training loss: 0.04507\n",
            "epoch: 31 w: 1.987 Training loss: 0.00175\n",
            "epoch: 41 w: 1.997 Training loss: 0.00007\n",
            "epoch: 51 w: 1.999 Training loss: 0.00000\n",
            "epoch: 61 w: 2.000 Training loss: 0.00000\n",
            "epoch: 71 w: 2.000 Training loss: 0.00000\n",
            "epoch: 81 w: 2.000 Training loss: 0.00000\n",
            "epoch: 91 w: 2.000 Training loss: 0.00000\n",
            "prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Automating the forward pass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn #pytorch neural network module\n",
        "\n",
        "#True function is y = 2*x\n",
        "x = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
        "\n",
        "num_samples, num_features = x.shape #each row is a sample and each column is a feature\n",
        "\n",
        "input_size = num_features\n",
        "output_size = num_features\n",
        "\n",
        "#using automated forward using pytorch\n",
        "model = nn.Linear(input_size, output_size) #one linear layer = Linear regression\n",
        "#it needs size of the input and size of the output as parameters\n",
        "\n",
        "test_input = torch.tensor([6], dtype=torch.float32)\n",
        "#test input for checking the predicition of the model\n",
        "#Also has to be a tensor\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "loss = nn.MSELoss() #we can call this function in the training loop\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "#instead of w list used before, we use this command\n",
        "\n",
        "\n",
        "print(f'prediction before training: f(5) = {model(test_input).item():.3f}') #formatted string can't directly print a tensor, you need to get the value of the  tensor.\n",
        "\n",
        "num_iters = 100\n",
        "for epoch in range(num_iters):\n",
        "   y_pred = model(x) #forward pass\n",
        "   l = loss(y,y_pred)\n",
        "  #  l.backward() #computes the d(loss)/dw\n",
        "  #  dw = w.grad\n",
        "  #  with torch.no_grad():\n",
        "  #     w -= lr*(dw) #don't want to compute the gradients for this part.\n",
        "  #  w.grad.zero_() #prevents accumulation of gradient before the next iteration.\n",
        "   l.backward() #although we do have the same command for computing the gradients.\n",
        "   optimizer.step() # for automating weight updates\n",
        "   optimizer.zero_grad() #zeroing the gradients.\n",
        "\n",
        "   if epoch % 10 == 0:\n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch: {epoch+1} w: {w[0][0].item():.3f} Training loss: {l:.5f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {model(test_input).item():.3f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SZXsC1qjf1sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925a4ca7-1051-4c2d-d7e3-6e4913036288"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = -0.121\n",
            "epoch: 1 w: 0.302 Training loss: 31.12153\n",
            "epoch: 11 w: 1.584 Training loss: 0.84659\n",
            "epoch: 21 w: 1.794 Training loss: 0.06089\n",
            "epoch: 31 w: 1.832 Training loss: 0.03830\n",
            "epoch: 41 w: 1.843 Training loss: 0.03557\n",
            "epoch: 51 w: 1.848 Training loss: 0.03349\n",
            "epoch: 61 w: 1.853 Training loss: 0.03154\n",
            "epoch: 71 w: 1.857 Training loss: 0.02970\n",
            "epoch: 81 w: 1.861 Training loss: 0.02798\n",
            "epoch: 91 w: 1.865 Training loss: 0.02635\n",
            "prediction after training: f(5) = 11.599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a custom model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn #pytorch neural network module\n",
        "\n",
        "#True function is y = 2*x\n",
        "x = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
        "\n",
        "num_samples, num_features = x.shape #each row is a sample and each column is a feature\n",
        "\n",
        "input_size = num_features\n",
        "output_size = num_features\n",
        "\n",
        "#using automated forward using pytorch\n",
        "model = nn.Linear(input_size, output_size) #one linear layer = Linear regression\n",
        "#it needs size of the input and size of the output as parameters\n",
        "\n",
        "#standard way of creating Pytorch model\n",
        "#understand this more:\n",
        "class LinearReg(nn.Module):\n",
        "  def __init__(self, input_dimension, output_dimension):\n",
        "    super(LinearReg, self).__init__() #super class\n",
        "    self.lin = nn.Linear(input_dimension, output_dimension)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearReg(input_size , output_size)\n",
        "\n",
        "\n",
        "test_input = torch.tensor([6], dtype=torch.float32)\n",
        "#test input for checking the predicition of the model\n",
        "#Also has to be a tensor\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "loss = nn.MSELoss() #we can call this function in the training loop\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "#instead of w list used before, we use this command\n",
        "\n",
        "\n",
        "print(f'prediction before training: f(5) = {model(test_input).item():.3f}') #formatted string can't directly print a tensor, you need to get the value of the  tensor.\n",
        "\n",
        "num_iters = 100\n",
        "for epoch in range(num_iters):\n",
        "  #forward pass\n",
        "   y_pred = model(x)\n",
        "   l = loss(y,y_pred)\n",
        "  #  l.backward() #computes the d(loss)/dw\n",
        "  #  dw = w.grad\n",
        "  #  with torch.no_grad():\n",
        "  #     w -= lr*(dw) #don't want to compute the gradients for this part.\n",
        "  #  w.grad.zero_() #prevents accumulation of gradient before the next iteration.\n",
        "   #backward pass\n",
        "   l.backward() #although we do have the same command for computing the gradients.\n",
        "   #weight update\n",
        "   optimizer.step() # for automating weight updates\n",
        "   optimizer.zero_grad() #zeroing the gradients.\n",
        "\n",
        "   if epoch % 10 == 0:\n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch: {epoch+1} w: {w[0][0].item():.3f} Training loss: {l:.5f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {model(test_input).item():.3f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r73PvQOVjGRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a1aab1-888c-48fa-d72f-af245be26252"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0.295\n",
            "epoch: 1 w: 0.495 Training loss: 32.67535\n",
            "epoch: 11 w: 1.802 Training loss: 0.84938\n",
            "epoch: 21 w: 2.011 Training loss: 0.02573\n",
            "epoch: 31 w: 2.044 Training loss: 0.00420\n",
            "epoch: 41 w: 2.048 Training loss: 0.00344\n",
            "epoch: 51 w: 2.047 Training loss: 0.00323\n",
            "epoch: 61 w: 2.046 Training loss: 0.00304\n",
            "epoch: 71 w: 2.044 Training loss: 0.00286\n",
            "epoch: 81 w: 2.043 Training loss: 0.00269\n",
            "epoch: 91 w: 2.042 Training loss: 0.00254\n",
            "prediction after training: f(5) = 12.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import total_ordering\n",
        "#linear regression using external datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_numpy, y_numpy = datasets.make_regression(n_samples=50, n_features=1, noise=10,random_state=1)\n",
        "x = torch.from_numpy(x_numpy.astype(np.float32)) #converting from numpy to torch\n",
        "print(x.shape)\n",
        "#print(x.dtype) #converted to float32 for tensor\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "print(y.shape) #right now, one row with 50 values\n",
        "y = y.view(y.shape[0],1) #reshaping the tensor for 1 column\n",
        "print(y.shape) #converted to 50 rows and 1 value per row (same shape as x now)\n",
        "num_samples, num_features = x.shape\n",
        "\n",
        "#model\n",
        "out_features = 1\n",
        "model = nn.Linear(num_features,out_features)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "#training loops\n",
        "total_epoch = 1000\n",
        "for epoch in range(total_epoch):\n",
        "  #forward pass\n",
        "  y_pred = model(x) #predicted tensor\n",
        "  l = criterion(y_pred, y)\n",
        "\n",
        "  #backward pass\n",
        "  l.backward() #computes the gradients\n",
        "  #weight update\n",
        "  optimizer.step() #performs weight updates\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #if (epoch+1)%10 == 0: #+1 as epoch is going from 0-99\n",
        "    #print(f'epoch: {epoch+1}, loss={l.item():.4f}, ')\n",
        "\n",
        "#plotting the predicted values\n",
        "predicted_values = model(x).detach().numpy() #First detach\n",
        "#to set requires grad = fales for converting to numpy array\n",
        "plt.plot(x_numpy,y_numpy, 'bo' )\n",
        "plt.plot(x_numpy,predicted_values, 'r') #red line for predicted values\n",
        "#plt.xlim(-2,2) #for some region specific plots\n",
        "#plt.ylim(-100,100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YX9oReqVjGTs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "0430b0fa-8c85-4b9d-a833-7d14420854da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 1])\n",
            "torch.Size([50])\n",
            "torch.Size([50, 1])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HlC74MosjGWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0xH_vhDjGZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}